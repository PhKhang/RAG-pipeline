{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f17d6f7",
   "metadata": {},
   "source": [
    "## Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b51591",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -q weaviate-client sentence-transformers pyvi pymupdf langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5a85c2",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "## Connecting to Weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21ad27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "\n",
    "client = weaviate.connect_to_local(\"localhost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bda436c",
   "metadata": {},
   "source": [
    "### Create Document Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dede28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate.classes.config import Configure, Property, DataType\n",
    "try:\n",
    "    client.collections.create(\n",
    "        name=\"Document\",\n",
    "        vectorizer_config=Configure.Vectorizer.none(),\n",
    "        properties=[Property(name=\"text\", data_type=DataType.TEXT)],\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error creating collection: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9144f3b8",
   "metadata": {},
   "source": [
    "### Closing the connection (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51fad60",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6d0fc4",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55f1137",
   "metadata": {},
   "source": [
    "### Test to check the distance of tokenized and non-tokenized (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a9a3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pyvi.ViTokenizer import tokenize\n",
    "\n",
    "\n",
    "# model = SentenceTransformer('dangvantuan/vietnamese-embedding')\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "raw = \"Hà Nội là thủ đô của Việt Nam\"\n",
    "tokenized = \"Hà_Nội là thủ_đô của Việt_Nam\"\n",
    "\n",
    "vec1 = model.encode(raw)\n",
    "vec2 = model.encode(tokenized)\n",
    "\n",
    "cos_sim = dot(vec1, vec2) / (norm(vec1) * norm(vec2))\n",
    "print(f\"Cosine similarity between raw and tokenized: {cos_sim:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab83b2c",
   "metadata": {},
   "source": [
    "### Embedding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52317afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def embed(text) -> List[List[float]]:\n",
    "    return model.encode(text).tolist()\n",
    "\n",
    "def import_texts_and_embeds_to_db(texts: List[str], embeddings: List[List],collection_name=\"Document\"):\n",
    "    for text, embedding in zip(texts, embeddings):\n",
    "        client.collections.get(collection_name).data.insert(\n",
    "            properties={\"text\": text}, vector=embedding\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233f5546",
   "metadata": {},
   "source": [
    "### Clear Document Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0528b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_document_collection():\n",
    "    client.collections.delete(\"Document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc60f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clear_document_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bfe340",
   "metadata": {},
   "source": [
    "### Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a581b393",
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate.classes.query import MetadataQuery\n",
    "\n",
    "query = \"Tinh thể?\"\n",
    "query_tokenized = tokenize(query)\n",
    "query_vector = model.encode(query_tokenized).tolist()\n",
    "\n",
    "result = client.collections.get(\"Document\").query.near_vector(\n",
    "    near_vector=query_vector,\n",
    "    limit=5,\n",
    "    return_metadata=MetadataQuery(distance=True)\n",
    ")\n",
    "\n",
    "retrieved_objects = [obj for obj in result.objects]\n",
    "\n",
    "print(\"Query results:\")\n",
    "for i, obj in enumerate(retrieved_objects, 1):\n",
    "    print(f\"{i}. Dist: {obj.metadata.distance} - {obj.properties['text'][:140]}...\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53d9ce9",
   "metadata": {},
   "source": [
    "## PDF to vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aef09b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"test-pdf/OS_C4_File and Disk management.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54594003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pymupdf\n",
    "from typing import List\n",
    "from langchain.schema import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "def pdf_to_raw_doc(file_name) -> List[Document]:\n",
    "    doc = pymupdf.open(file_name)\n",
    "    pages: List[Document] = []\n",
    "    for pg_num, page in enumerate(doc, start=1):\n",
    "        pages.append(\n",
    "            Document(\n",
    "                page_content=page.get_text(\"text\"),\n",
    "                metadata={\"source\": file_name, \"page\": pg_num},\n",
    "            )\n",
    "        )\n",
    "    return pages\n",
    "\n",
    "\n",
    "def split_doc(doc: Document, chunk_size: int, chunk_overlap: int) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Splits a Document into smaller chunks based on the specified chunk size and overlap.\n",
    "    \"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    )\n",
    "\n",
    "    page_contents = splitter.split_text(doc.page_content)\n",
    "\n",
    "    splitted_docs: List[Document] = []\n",
    "    for i, page_content in enumerate(page_contents):\n",
    "        splitted_docs.extend(\n",
    "            Document(\n",
    "                page_content=page_content,\n",
    "                metadata={\n",
    "                    \"source\": doc.metadata.get(\"source\", \"\"),\n",
    "                    \"page\": doc.metadata.get(\"page\", 1),\n",
    "                    \"chunk_index\": i,\n",
    "                },\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return splitted_docs\n",
    "\n",
    "\n",
    "def save_to_json(data, output_file):\n",
    "    \"\"\"Save the processed data to a JSON file.\"\"\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "def docs_to_json(docs: List[Document]) -> dict:\n",
    "    \"\"\"\n",
    "    Including preprocessing and chunking.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        {\n",
    "            \"source\": doc.metadata.get(\"source\", \"\"),\n",
    "            \"page\": doc.metadata.get(\"page\", 1),\n",
    "            \"chunk_index\": doc.metadata.get(\"chunk_index\", 0),\n",
    "            \"content\": doc.page_content,\n",
    "        }\n",
    "        for doc in docs\n",
    "    ]\n",
    "\n",
    "\n",
    "def json_to_docs(file_name: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load documents from a JSON file.\n",
    "    \"\"\"\n",
    "    with open(file_name, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return [\n",
    "        Document(\n",
    "            page_content=item[\"content\"],\n",
    "            metadata={\n",
    "                \"source\": item.get(\"source\", \"\"),\n",
    "                \"page\": item.get(\"page\", 1),\n",
    "                \"chunk_index\": item.get(\"chunk_index\", None),\n",
    "            },\n",
    "        )\n",
    "        for item in data\n",
    "    ]\n",
    "\n",
    "\n",
    "def docs_to_strings(docs: List[Document]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Convert a list of Document objects to a list of strings.\n",
    "    \"\"\"\n",
    "    return [str(doc) for doc in docs]\n",
    "\n",
    "\n",
    "raw_docs = pdf_to_raw_doc(file_name)\n",
    "processed_docs: List[Document] = []\n",
    "for doc in raw_docs:\n",
    "    if len(doc.page_content) > 800:\n",
    "        print(\"Split\")\n",
    "        sub_docs = split_doc(doc, chunk_size=800, chunk_overlap=100)\n",
    "        processed_docs.extend(sub_docs)\n",
    "    else:\n",
    "        processed_docs.append(doc)\n",
    "\n",
    "json_docs = docs_to_json(processed_docs)\n",
    "save_to_json(json_docs, \"json/raw_docs.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76c251f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embed(docs_to_strings(processed_docs))\n",
    "import_texts_and_embeds_to_db(\n",
    "    docs_to_strings(processed_docs), embeddings, collection_name=\"Document\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ef9503",
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate.classes.query import MetadataQuery\n",
    "\n",
    "query = \"Scheduling?\"\n",
    "query_tokenized = tokenize(query)\n",
    "query_vector = model.encode(query).tolist()\n",
    "\n",
    "result = client.collections.get(\"Document\").query.near_vector(\n",
    "    near_vector=query_vector,\n",
    "    limit=5,\n",
    "    return_metadata=MetadataQuery(distance=True)\n",
    ")\n",
    "\n",
    "retrieved_objects = [obj for obj in result.objects]\n",
    "\n",
    "print(\"Query results:\")\n",
    "for i, obj in enumerate(retrieved_objects, 1):\n",
    "    print(f\"{i}. Dist: {obj.metadata.distance}:\\n {obj.properties['text'][:240]}...\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb5609d",
   "metadata": {},
   "source": [
    "### Check Document Collection size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e834e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear_document_collection()\n",
    "client.collections.get(\"Document\").aggregate.over_all(total_count=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
